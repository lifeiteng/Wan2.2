# Copyright      2025                          (authors: Feiteng Li)
import gradio as gr
import os
import sys
import warnings
import random
import tempfile
import subprocess
from datetime import datetime
from pathlib import Path
import torch
from PIL import Image
from torchvision.utils import save_image

warnings.filterwarnings('ignore')

import wan
from wan.configs import MAX_AREA_CONFIGS, SIZE_CONFIGS, SUPPORTED_SIZES, WAN_CONFIGS
from wan.utils.utils import save_video

class WanVideoGenerator:
    def __init__(self):
        self.current_pipeline = None
        self.current_task = None
        self.current_ckpt_dir = None

    def _init_pipeline(self, task, ckpt_dir, device_id=0, rank=0, t5_cpu=True, convert_model_dtype=True, progress_callback=None):
        """åˆå§‹åŒ–æˆ–é‡æ–°åˆå§‹åŒ–pipeline"""
        # å¦‚æœtaskæˆ–ckpt_dirå‘ç”Ÿå˜åŒ–ï¼Œæ¸…é™¤å½“å‰pipeline
        if (self.current_task != task or 
            self.current_ckpt_dir != ckpt_dir or 
            self.current_pipeline is None):

            # æ¸…ç†ä¹‹å‰çš„pipeline
            if self.current_pipeline is not None:
                if progress_callback:
                    progress_callback(0.17, desc="æ¸…ç†æ—§æ¨¡å‹...")
                del self.current_pipeline
                torch.cuda.empty_cache()

            if progress_callback:
                progress_callback(0.18, desc="åŠ è½½æ¨¡å‹é…ç½®...")
            
            # è·å–é…ç½®
            cfg = WAN_CONFIGS[task]

            if progress_callback:
                progress_callback(0.20, desc="åˆ›å»ºæ¨¡å‹å®ä¾‹...")

            # æ ¹æ®ä»»åŠ¡ç±»å‹åˆ›å»ºç›¸åº”çš„pipeline
            if "t2v" in task:
                self.current_pipeline = wan.WanT2V(
                    config=cfg,
                    checkpoint_dir=ckpt_dir,
                    device_id=device_id,
                    rank=rank,
                    t5_fsdp=False,
                    dit_fsdp=False,
                    use_sp=False,
                    t5_cpu=t5_cpu,
                    convert_model_dtype=convert_model_dtype,
                )
            elif "ti2v" in task:
                self.current_pipeline = wan.WanTI2V(
                    config=cfg,
                    checkpoint_dir=ckpt_dir,
                    device_id=device_id,
                    rank=rank,
                    t5_fsdp=False,
                    dit_fsdp=False,
                    use_sp=False,
                    t5_cpu=t5_cpu,
                    convert_model_dtype=convert_model_dtype,
                )
            else:  # i2v-A14B
                self.current_pipeline = wan.WanI2V(
                    config=cfg,
                    checkpoint_dir=ckpt_dir,
                    device_id=device_id,
                    rank=rank,
                    t5_fsdp=False,
                    dit_fsdp=False,
                    use_sp=False,
                    t5_cpu=t5_cpu,
                    convert_model_dtype=convert_model_dtype,
                )

            # æ›´æ–°å½“å‰çŠ¶æ€
            self.current_task = task
            self.current_ckpt_dir = ckpt_dir
            
            if progress_callback:
                progress_callback(0.24, desc="æ¨¡å‹åŠ è½½å®Œæˆ")

    def generate_video(self, task, size, prompt, image, ckpt_dir, 
                      sample_steps, sample_shift, sample_guide_scale, 
                      frame_num, base_seed, offload_model, t5_cpu, 
                      convert_model_dtype, progress=gr.Progress()):
        """
        ç”Ÿæˆè§†é¢‘çš„ä¸»è¦å‡½æ•°
        """
        try:
            progress(0.05, desc="éªŒè¯å‚æ•°...")

            # éªŒè¯å‚æ•°
            if not ckpt_dir or not os.path.exists(ckpt_dir):
                return None, "é”™è¯¯ï¼šè¯·æä¾›æœ‰æ•ˆçš„æ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½•"

            if not prompt.strip():
                return None, "é”™è¯¯ï¼šè¯·è¾“å…¥æç¤ºè¯"

            if task == "i2v-A14B" and image is None:
                return None, "é”™è¯¯ï¼šI2Vä»»åŠ¡éœ€è¦è¾“å…¥å›¾åƒ"

            progress(0.1, desc="å‡†å¤‡é…ç½®...")

            # è·å–é…ç½®
            cfg = WAN_CONFIGS[task]

            # è®¾ç½®é»˜è®¤å€¼
            if sample_steps == 0:
                sample_steps = cfg.sample_steps
            if sample_shift == 0:
                sample_shift = cfg.sample_shift
            if sample_guide_scale == 0:
                sample_guide_scale = cfg.sample_guide_scale
            if frame_num == 0:
                frame_num = cfg.frame_num
            if base_seed == -1:
                base_seed = random.randint(0, sys.maxsize)

            progress(0.15, desc="åˆå§‹åŒ–æ¨¡å‹...")

            # åˆå§‹åŒ–æˆ–é‡ç”¨pipeline
            self._init_pipeline(task, ckpt_dir, device_id=0, rank=0, 
                              t5_cpu=t5_cpu, convert_model_dtype=convert_model_dtype,
                              progress_callback=progress)

            progress(0.25, desc="å‡†å¤‡ç”Ÿæˆå‚æ•°...")

            # ä½¿ç”¨å½“å‰pipelineç”Ÿæˆè§†é¢‘
            if "t2v" in task:
                progress(0.3, desc=f"T2Væ¨ç†ä¸­ (å…±{sample_steps}æ­¥)...")
                video = self.current_pipeline.generate(
                    prompt,
                    size=SIZE_CONFIGS[size],
                    frame_num=frame_num,
                    shift=sample_shift,
                    sample_solver='unipc',
                    sampling_steps=sample_steps,
                    guide_scale=sample_guide_scale,
                    seed=base_seed,
                    offload_model=offload_model
                )

            elif "ti2v" in task:
                # å¤„ç†è¾“å…¥å›¾åƒï¼ˆå¯é€‰ï¼‰
                img = None
                if image is not None:
                    progress(0.3, desc="å¤„ç†è¾“å…¥å›¾åƒ...")
                    img = Image.open(image).convert("RGB") if isinstance(image, str) else image
                    progress(0.32, desc="å›¾åƒé¢„å¤„ç†å®Œæˆ")

                progress(0.35, desc=f"TI2Væ¨ç†ä¸­ (å…±{sample_steps}æ­¥)...")
                video = self.current_pipeline.generate(
                    prompt,
                    img=img,
                    size=SIZE_CONFIGS[size],
                    max_area=MAX_AREA_CONFIGS[size],
                    frame_num=frame_num,
                    shift=sample_shift,
                    sample_solver='unipc',
                    sampling_steps=sample_steps,
                    guide_scale=sample_guide_scale,
                    seed=base_seed,
                    offload_model=offload_model
                )

            else:  # i2v-A14B
                # å¤„ç†è¾“å…¥å›¾åƒ
                progress(0.3, desc="å¤„ç†è¾“å…¥å›¾åƒ...")
                img = Image.open(image).convert("RGB") if isinstance(image, str) else image
                progress(0.32, desc="å›¾åƒé¢„å¤„ç†å®Œæˆ")

                progress(0.35, desc=f"I2Væ¨ç†ä¸­ (å…±{sample_steps}æ­¥)...")
                video = self.current_pipeline.generate(
                    prompt,
                    img,
                    max_area=MAX_AREA_CONFIGS[size],
                    frame_num=frame_num,
                    shift=sample_shift,
                    sample_solver='unipc',
                    sampling_steps=sample_steps,
                    guide_scale=sample_guide_scale,
                    seed=base_seed,
                    offload_model=offload_model
                )

            progress(0.8, desc="æ¨ç†å®Œæˆï¼Œå‡†å¤‡ä¿å­˜...")

            progress(0.85, desc="ä¿å­˜è§†é¢‘æ–‡ä»¶...")

            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            formatted_time = datetime.now().strftime("%Y%m%d_%H%M%S")
            formatted_prompt = prompt.replace(" ", "_").replace("/", "_")[:50]
            
            # æ ¹æ®å¸§æ•°å†³å®šä¿å­˜æ ¼å¼
            if frame_num == 1:
                output_path = f"{task}_{size.replace('*','x')}_{formatted_prompt}_{formatted_time}.png"
                progress(0.9, desc="ä¿å­˜ä¸ºPNGå›¾åƒ...")
            else:
                output_path = f"{task}_{size.replace('*','x')}_{formatted_prompt}_{formatted_time}.mp4"
                progress(0.9, desc="ç¼–ç è§†é¢‘...")

            # æ ¹æ®å¸§æ•°ä¿å­˜ä¸ºä¸åŒæ ¼å¼
            if frame_num == 1:
                # ä¿å­˜ä¸ºPNGå›¾åƒ
                save_image(
                    video[0],
                    output_path,
                    normalize=True,
                    value_range=(-1, 1)
                )
            else:
                # ä¿å­˜ä¸ºMP4è§†é¢‘
                save_video(
                    tensor=video[None],
                    save_file=output_path,
                    fps=cfg.sample_fps,
                    nrow=1,
                    normalize=True,
                    value_range=(-1, 1)
                )

            progress(0.95, desc="æ¸…ç†å†…å­˜...")

            progress(1.0, desc="ç”Ÿæˆå®Œæˆ!")

            # æ¸…ç†è§†é¢‘å†…å­˜ï¼ˆä½†ä¿ç•™pipelineï¼‰
            del video
            torch.cuda.empty_cache()

            # æ ¹æ®æ–‡ä»¶æ ¼å¼è¿”å›ä¸åŒçš„æˆåŠŸä¿¡æ¯
            file_type = "å›¾åƒ" if frame_num == 1 else "è§†é¢‘"
            return output_path, f"{file_type}ç”ŸæˆæˆåŠŸï¼ç§å­å€¼: {base_seed}, ä¿å­˜è‡³: {output_path}"

        except Exception as e:
            return None, f"ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"

def create_gradio_interface():
    generator = WanVideoGenerator()

    # ç¤ºä¾‹æç¤ºè¯
    example_prompts = {
        "t2v-A14B": "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage.",
        "i2v-A14B": "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression.",
        "ti2v-5B": "5 ç§’ã€16âˆ¶9 ç”»å¹…ã€24 fps çš„èˆªæ‹é•œå¤´ã€‚æœºä½åœ¨è·åœ°é¢çº¦ 2 ç±³å¤„å¿«é€Ÿå‰ç§»ï¼Œä¿¯è§†å¹¶è·Ÿæ‹ä¸€å 23 å²ã€ä½“æ ¼å¥ç¾çš„å¥³æ€§ã€‚"
    }

    def update_size_choices(task):
        return gr.Dropdown(choices=SUPPORTED_SIZES[task], value=SUPPORTED_SIZES[task][0])

    def update_prompt_example(task):
        return example_prompts.get(task, "")

    def update_image_visibility(task):
        return gr.Image(visible=task in ["i2v-A14B", "ti2v-5B"])
    
    def generate_video_wrapper(task, size, prompt, image, ckpt_dir, 
                              sample_steps, sample_shift, sample_guide_scale, 
                              frame_num, base_seed, offload_model, t5_cpu, 
                              convert_model_dtype, progress=gr.Progress()):
        """åŒ…è£…å‡½æ•°ï¼Œå¤„ç†è¾“å‡ºæ˜¾ç¤º"""
        result_path, info = generator.generate_video(
            task, size, prompt, image, ckpt_dir,
            sample_steps, sample_shift, sample_guide_scale,
            frame_num, base_seed, offload_model, t5_cpu,
            convert_model_dtype, progress
        )
        
        if result_path is None:
            # ç”Ÿæˆå¤±è´¥
            return None, None, gr.Video(visible=True), gr.Image(visible=False), info
        
        if frame_num == 1:
            # å•å¸§ï¼Œæ˜¾ç¤ºå›¾åƒ
            return None, result_path, gr.Video(visible=False), gr.Image(visible=True), info
        else:
            # å¤šå¸§ï¼Œæ˜¾ç¤ºè§†é¢‘
            return result_path, None, gr.Video(visible=True), gr.Image(visible=False), info

    with gr.Blocks(title="Wan Video Generator", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¬ Wan Video Generator

        åŸºäº Wan æ¨¡å‹çš„è§†é¢‘ç”Ÿæˆå·¥å…·ï¼Œæ”¯æŒæ–‡æœ¬åˆ°è§†é¢‘(T2V)ã€å›¾åƒåˆ°è§†é¢‘(I2V)å’Œæ–‡æœ¬+å›¾åƒåˆ°è§†é¢‘(TI2V)ç”Ÿæˆã€‚
        """)

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### åŸºç¡€è®¾ç½®")

                task = gr.Dropdown(
                    choices=list(WAN_CONFIGS.keys()),
                    value="ti2v-5B",
                    label="ä»»åŠ¡ç±»å‹",
                    info="é€‰æ‹©ç”Ÿæˆä»»åŠ¡ç±»å‹"
                )

                size = gr.Dropdown(
                    choices=SUPPORTED_SIZES["ti2v-5B"],
                    value="1280*704",
                    label="è§†é¢‘å°ºå¯¸",
                    info="ç”Ÿæˆè§†é¢‘çš„åˆ†è¾¨ç‡"
                )

                ckpt_dir = gr.Textbox(
                    value="./Wan2.2-TI2V-5B",
                    label="æ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½•",
                    info="æ¨¡å‹æƒé‡æ–‡ä»¶çš„è·¯å¾„"
                )

                prompt = gr.Textbox(
                    lines=4,
                    value=example_prompts["ti2v-5B"],
                    label="æç¤ºè¯",
                    info="æè¿°è¦ç”Ÿæˆçš„è§†é¢‘å†…å®¹"
                )

                image = gr.Image(
                    type="pil",
                    label="è¾“å…¥å›¾åƒ (I2Vä»»åŠ¡å¿…éœ€ï¼ŒTI2Vä»»åŠ¡å¯é€‰)",
                    visible=True
                )

            with gr.Column(scale=1):
                gr.Markdown("### é«˜çº§å‚æ•°")

                with gr.Row():
                    sample_steps = gr.Slider(
                        minimum=1,
                        maximum=100,
                        value=50,
                        step=1,
                        label="é‡‡æ ·æ­¥æ•°",
                        info="é»˜è®¤å€¼=50ï¼Œé‡‡æ ·æ­¥æ•°è¶Šå¤šï¼Œç”Ÿæˆçš„è§†é¢‘è´¨é‡è¶Šé«˜ï¼Œä½†æ—¶é—´ä¹Ÿè¶Šé•¿"
                    )

                    frame_num = gr.Slider(
                        minimum=1,
                        maximum=200,
                        value=121,
                        step=4,
                        label="å¸§æ•°",
                        info="ç”Ÿæˆè§†é¢‘çš„å¸§æ•°ï¼Œé»˜è®¤å€¼=121"
                    )

                with gr.Row():
                    sample_shift = gr.Slider(
                        minimum=0.0,
                        maximum=10.0,
                        value=0,
                        step=0.1,
                        label="é‡‡æ ·åç§»",
                        info="æµåŒ¹é…è°ƒåº¦å™¨çš„é‡‡æ ·åç§»å› å­ï¼Œ0è¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼"
                    )

                    sample_guide_scale = gr.Slider(
                        minimum=1.0,
                        maximum=20.0,
                        value=0,
                        step=0.1,
                        label="å¼•å¯¼å°ºåº¦",
                        info="åˆ†ç±»å™¨æ— å…³å¼•å¯¼å°ºåº¦ï¼Œ0è¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼"
                    )

                base_seed = gr.Number(
                    value=-1,
                    label="éšæœºç§å­",
                    info="ç”Ÿæˆçš„éšæœºç§å­ï¼Œ-1è¡¨ç¤ºéšæœºç”Ÿæˆ"
                )

                with gr.Row():
                    offload_model = gr.Checkbox(
                        value=True,
                        label="æ¨¡å‹å¸è½½",
                        info="æ¯æ¬¡å‰å‘åå°†æ¨¡å‹å¸è½½åˆ°CPUä»¥å‡å°‘GPUå†…å­˜ä½¿ç”¨"
                    )

                    t5_cpu = gr.Checkbox(
                        value=True,
                        label="T5ä½¿ç”¨CPU",
                        info="å°†T5æ¨¡å‹æ”¾åœ¨CPUä¸Šè¿è¡Œ"
                    )

                    convert_model_dtype = gr.Checkbox(
                        value=True,
                        label="è½¬æ¢æ¨¡å‹ç²¾åº¦",
                        info="è½¬æ¢æ¨¡å‹å‚æ•°çš„æ•°æ®ç±»å‹"
                    )

                generate_btn = gr.Button(
                    "ğŸ¬ ç”Ÿæˆè§†é¢‘",
                    variant="primary",
                    size="lg"
                )

        with gr.Row():
            with gr.Column():
                output_video = gr.Video(
                    label="ç”Ÿæˆçš„è§†é¢‘",
                    height=400,
                    visible=True
                )
                
                output_image = gr.Image(
                    label="ç”Ÿæˆçš„å›¾åƒ",
                    height=400,
                    visible=False
                )

                output_info = gr.Textbox(
                    label="ç”Ÿæˆä¿¡æ¯",
                    interactive=False
                )

        # äº‹ä»¶å¤„ç†
        task.change(
            fn=update_size_choices,
            inputs=[task],
            outputs=[size]
        )

        task.change(
            fn=update_prompt_example,
            inputs=[task],
            outputs=[prompt]
        )

        task.change(
            fn=update_image_visibility,
            inputs=[task],
            outputs=[image]
        )

        generate_btn.click(
            fn=generate_video_wrapper,
            inputs=[
                task, size, prompt, image, ckpt_dir,
                sample_steps, sample_shift, sample_guide_scale,
                frame_num, base_seed, offload_model, t5_cpu,
                convert_model_dtype
            ],
            outputs=[output_video, output_image, output_video, output_image, output_info],
            show_progress=True
        )

        # ç¤ºä¾‹
        gr.Markdown("### ğŸ“ ä½¿ç”¨è¯´æ˜")
        gr.Markdown("""
        1. **é€‰æ‹©ä»»åŠ¡ç±»å‹**ï¼š
           - `t2v-A14B`: æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆ
           - `i2v-A14B`: å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆï¼ˆéœ€è¦ä¸Šä¼ å›¾åƒï¼‰
           - `ti2v-5B`: æ–‡æœ¬+å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆï¼ˆå›¾åƒå¯é€‰ï¼‰

        2. **è®¾ç½®å‚æ•°**ï¼š
           - æ ¹æ®éœ€è¦è°ƒæ•´è§†é¢‘å°ºå¯¸ã€é‡‡æ ·å‚æ•°ç­‰
           - é«˜çº§å‚æ•°ä¸­çš„0å€¼è¡¨ç¤ºä½¿ç”¨æ¨¡å‹é»˜è®¤è®¾ç½®

        3. **è¾“å…¥å†…å®¹**ï¼š
           - è¾“å…¥æè¿°æ€§çš„æç¤ºè¯
           - å¯¹äºI2Vä»»åŠ¡ï¼Œå¿…é¡»ä¸Šä¼ å›¾åƒ
           - å¯¹äºTI2Vä»»åŠ¡ï¼Œå¯ä»¥é€‰æ‹©ä¸Šä¼ å›¾åƒæˆ–ä»…ä½¿ç”¨æ–‡æœ¬

        4. **ç‚¹å‡»ç”Ÿæˆ**ï¼šç­‰å¾…æ¨¡å‹å¤„ç†å®Œæˆ

        **æ³¨æ„**ï¼šé¦–æ¬¡è¿è¡Œéœ€è¦åŠ è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ã€‚ç¡®ä¿æœ‰è¶³å¤Ÿçš„GPUå†…å­˜ã€‚
        """)

    return demo

if __name__ == "__main__":
    # åˆ›å»ºGradioç•Œé¢
    demo = create_gradio_interface()

    # å¯åŠ¨æœåŠ¡
    demo.launch(
        server_name="0.0.0.0",  # å…è®¸å¤–éƒ¨è®¿é—®
        server_port=7860,       # ç«¯å£
        share=False,            # æ˜¯å¦åˆ›å»ºå…¬å…±é“¾æ¥
        debug=True,             # è°ƒè¯•æ¨¡å¼
        show_error=True         # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
    )
